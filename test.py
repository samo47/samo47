from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

# ------ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ------
model_name = "aubmindlab/aragpt2-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
chatbot = pipeline('text-generation', model=model, tokenizer=tokenizer)

# ------ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ------
MAX_HISTORY = 3  # Ø¹Ø¯Ø¯ Ø§Ù„Ø¬ÙˆÙ„Ø§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø© (User + Bot = Ø¬ÙˆÙ„Ø© ÙˆØ§Ø­Ø¯Ø©)
conversation_history = []

def update_memory(user_input, bot_response):
    conversation_history.extend([
        f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_input}",
        f"Ø§Ù„Ø¨ÙˆØª: {bot_response}"
    ])
    # Ø­Ø°Ù Ø£Ù‚Ø¯Ù… Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ø¹Ù†Ø¯ ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯
    while len(conversation_history) > MAX_HISTORY * 2:
        conversation_history.pop(0)

def generate_response(user_input):
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    context = "\n".join(conversation_history) + f"\nØ§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_input}\nØ§Ù„Ø¨ÙˆØª:"
    
    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯
    response = chatbot(
        context,
        max_length=300,
        num_return_sequences=1,
        pad_token_id=tokenizer.eos_token_id
    )
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ø¯ Ø§Ù„Ø£Ø®ÙŠØ± ÙÙ‚Ø·
    full_text = response[0]['generated_text']
    new_response = full_text.split("Ø§Ù„Ø¨ÙˆØª:")[-1].strip().split('\n')[0]
    
    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    update_memory(user_input, new_response)
    
    return new_response

# ------ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ------
print("Ù…Ø±Ø­Ø¨Ù‹Ø§! Ø£Ù†Ø§ Ø¨ÙˆØª Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© Ø§Ù„Ø°ÙƒÙŠ (Ø£ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡)")
while True:
    user_input = input("Ø£Ù†Øª: ")
    
    if user_input.lower() in ['Ø®Ø±ÙˆØ¬', 'exit', 'quit']:
        print("Ø§Ù„Ø¨ÙˆØª: Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡! ğŸ‘‹")
        break
        
    response = generate_response(user_input)
    print(f"Ø§Ù„Ø¨ÙˆØª: {response}")